{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNz4Eld+N/SwQzb1Ph3id0b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qidopox/Deep_Learning_for_image_processing_practice/blob/main/DL_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Deep Learning for image processing practice"
      ],
      "metadata": {
        "id": "xY_W7Qy8mm0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural network (CNN) was usually used for image processing. Common applications include image classification, segmentation and denoising. In this practice, we are going to train a neural network for classifying handwritten numbers （MNIST dataset）. We are going to use two different trained neural networks for two separate tasks - image segmentation and denoising."
      ],
      "metadata": {
        "id": "0okQpRpVtaps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 1: Train a neural network for classifying handwritten numbers\n",
        "\n",
        "In this exercise, we will use MNIST database. The MNIST database (Modified National Institute of Standards and Technology database) is a collection of handwritten digits. For details of the dataset, please see https://paperswithcode.com/dataset/mnist\n",
        "\n",
        "This exercise is manageable using CPU time. As a free Colab account would have limited GPU runtime, I would like to recommend you to do this exercise using CPU.\n",
        "\n",
        "To change runtime type, on the top left option bar, select *Runtime* --> *Change runtime type*."
      ],
      "metadata": {
        "id": "ktQyeTeHxbY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first import the relevant python packages. In this case, we will use tensorflow and keras to build and train neural networks. The matlotlib package is for figure plotting. The datatime package is to provide access to date and time."
      ],
      "metadata": {
        "id": "FLz824OPd5yS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4OTrxxGlB8j"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST dataset was included in the keras dataset and thus can be loaded directly from keras.\n",
        "\n",
        "As tensorflow takes float32 as the input, we need to convert the input images, which are in uint8 format, to float32."
      ],
      "metadata": {
        "id": "xRfiwXj7epxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = tf.cast(x_train, tf.float32) / 255\n",
        "x_test = tf.cast(x_test, tf.float32) / 255"
      ],
      "metadata": {
        "id": "D-N-7B4iFLBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing out the sizes of the training dataset and test dataset. There are 60,000 examples in the training dataset and 10,000 in the test dataset. The image inputs are in the sizes of 28 by 28."
      ],
      "metadata": {
        "id": "jTlEHhqEfOBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('training data input shape:',x_train.shape,'\\n training data label shape',y_train.shape,'\\n test data input shape:',x_test.shape,'\\n test data label shape:',y_test.shape)"
      ],
      "metadata": {
        "id": "GStP2KLjJlLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting out an example from the training dataset and check its corresponding label is correct."
      ],
      "metadata": {
        "id": "8poDwMIDfmHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "plt.imshow(x_train[i,:,:])\n",
        "plt.show()\n",
        "print('label of the figure is ', y_train[i])"
      ],
      "metadata": {
        "id": "KlATCX2Qtpvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct a neural network with an input the same size as the images and an output equals to 10 which corresponds to the 10 different digits, 0-9."
      ],
      "metadata": {
        "id": "c8U9uDVRfkci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = inputs = keras.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
        "x = keras.layers.Flatten()(inputs)\n",
        "x = keras.layers.Dense(64, activation=tf.nn.relu)(x)\n",
        "outputs = keras.layers.Dense(10, activation=tf.nn.softmax)(x)\n",
        "model= keras.Model(inputs=inputs, outputs=outputs,name='mnist_classification_fully_corrected')"
      ],
      "metadata": {
        "id": "5pIw3EYeM_Rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can print out a summary of the neural network."
      ],
      "metadata": {
        "id": "r9wDYzPxryfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "NNLUpdrrRz_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We select \"Adam\" as the optimisation algorithm for the neural network training. The loss function is \"SparseCategoricalCrossentropy\". We also use tensorboard to monitor the training process.\n",
        "\n",
        "For details of \"Adam\", please find https://arxiv.org/abs/1412.6980\n",
        "\n",
        "For details of \"SparseCategoricalCrossentropy\", please find https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy"
      ],
      "metadata": {
        "id": "We9OzK8Sr8tI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],)\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "Ggp6HuAuRX8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We fit x_train dataset as the input training dataset and y_train as the output training dataset. We select to train a maximum of 30 epochs. We fit x_test dataset as the input validation dataset and y_test as the output validation dataset.\n",
        "\n",
        "By running this cell, you will start to train your neural network."
      ],
      "metadata": {
        "id": "CIW9zgU9uJwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    x=x_train,y=y_train,\n",
        "    epochs=10,\n",
        "    validation_data=(x_test,y_test),\n",
        "    callbacks=[tensorboard_callback],\n",
        ")"
      ],
      "metadata": {
        "id": "-mNMokx9z3Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open tensorboard to monitor the network training."
      ],
      "metadata": {
        "id": "OZ9aWRH5Mbwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "B2qb794IN4Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The trained network model is saved in the folder \"models\" as a h5 file."
      ],
      "metadata": {
        "id": "YrfiwkNL6uAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('./models/'+model.name+'_.h5')"
      ],
      "metadata": {
        "id": "XFRKIInKuyKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the trained model. Using the trained model to classify the digit from handwritten number."
      ],
      "metadata": {
        "id": "uxijpq26QSrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model('./models/'+model.name+'_.h5', compile=False)\n",
        "i = 1\n",
        "y_pred=np.argmax(model.predict(np.asarray(x_test[i,:,:]).reshape([1,28,28,1])))\n",
        "\n",
        "print('label:',y_test[i],'\\n prediction:',y_pred)"
      ],
      "metadata": {
        "id": "v4vNeQ217X1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 2: Train a convolutional neural network working on MNIST dataset for classifying handwritten numbers\n",
        "\n",
        "This exercise is manageable using CPU time.\n",
        "\n",
        "Let's now construct a CNN for the same task. Please compare the architectures and the performances of the two types of networks."
      ],
      "metadata": {
        "id": "8F3sa4lMUmMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(x_train.shape[1], x_train.shape[2],1))\n",
        "x = keras.layers.Conv2D(4,(3, 3),activation=tf.nn.relu,\n",
        "                  kernel_initializer=\"glorot_uniform\",\n",
        "                  padding=\"same\",name=\"conv1\",)(inputs)\n",
        "x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"pool1\")(x)\n",
        "x = keras.layers.Conv2D(8,(3, 3),activation=tf.nn.relu,\n",
        "                  kernel_initializer=\"glorot_uniform\",\n",
        "                  padding=\"same\",name=\"conv2\",)(x)\n",
        "x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"pool2\")(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "x = keras.layers.Dense(16, activation=tf.nn.relu)(x)\n",
        "outputs = keras.layers.Dense(10, activation=tf.nn.softmax)(x)\n",
        "model_CNN = keras.Model(inputs=inputs, outputs=outputs,name='mnist_classification_CNN')\n"
      ],
      "metadata": {
        "id": "CfsdjEkh6AHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_CNN.summary()"
      ],
      "metadata": {
        "id": "dDg5roRF90eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_CNN.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],)\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "fgQzkLYG93u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_CNN.fit(\n",
        "    x=x_train,y=y_train,\n",
        "    epochs=10,\n",
        "    validation_data=(x_test,y_test),\n",
        "    callbacks=[tensorboard_callback],\n",
        ")"
      ],
      "metadata": {
        "id": "jOGuAl4WAVoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Please take a look at the loss function plot (epoch_loss) on the tensorboard. What are the differences of the two plots? Why do you think the differences?\n",
        " If you repeat the process and retrain the network, do you obtain the same result plots on tensorboard?"
      ],
      "metadata": {
        "id": "Q_Uczcq541Ji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 3: Train a U-net for image segmentation through transfer learning\n",
        "\n",
        "In this practice, we use the Oxford-IIIT Pet Dataset (Parkhi et al, 2012). The dataset consists of images of 37 pet breeds. Please see details here https://www.robots.ox.ac.uk/%7Evgg/data/pets/\n",
        "\n",
        "This Exercise was adapted from the tensorflow tutorial https://www.tensorflow.org/tutorials/images/segmentation\n",
        "\n",
        "You may wish to switch your runtime to GPU for this exercise.\n",
        "\n"
      ],
      "metadata": {
        "id": "DqLrWdV_Ymkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "dmckYV6FBTp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you work on this practice in the same runtime as the first practice, you will not need to re-import the following packages."
      ],
      "metadata": {
        "id": "OQqkdYeHkzcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime"
      ],
      "metadata": {
        "id": "uZq3afGUlGt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Oxford-IIIT Pet Dataset. It may take a while."
      ],
      "metadata": {
        "id": "zZQBqCpblNAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)"
      ],
      "metadata": {
        "id": "NxFGNYkqlMKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the data info."
      ],
      "metadata": {
        "id": "r4mNa4n0-Fao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(info)"
      ],
      "metadata": {
        "id": "Wz9FTUsjuXAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the data for batch training. There are two functions and one class we defined here:\n",
        "\n",
        "1. Normalise the data so that the pixel values of the images fall in the range of [0,1]. The pixel values in the masks for image segmentations were labeled either {1,2,3}. It is easier to work in python if the labels are {0,1,2} and thus we deduct 1 to all pixels in the masks.\n",
        "\n",
        "2. Load the images. Tensorflow dataset has a non-specified image size that allow users to define. We set the input images and masks to have sizes of 128 by 128.\n",
        "\n",
        "3. Data augmentation. We do some simple data augmentation by flip and rotate the images and masks.\n",
        "\n",
        "Finally we define training batch and test batch to feed into network training.\n",
        "\n",
        "* Note that for those who are not too familiar with python, *def* and *class* can be seen as two styles of python programming and differences were subtle. *def* is task oriented and *class* is data oriented."
      ],
      "metadata": {
        "id": "GwdYS2qx-NfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(input_image, input_mask):\n",
        "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
        "  input_mask -= 1\n",
        "  return input_image, input_mask\n",
        "\n",
        "def load_image(datapoint):\n",
        "  input_image = tf.image.resize(datapoint['image'], (128, 128))\n",
        "  input_mask = tf.image.resize(\n",
        "    datapoint['segmentation_mask'],\n",
        "    (128, 128),\n",
        "    method = tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n",
        "  )\n",
        "\n",
        "  input_image, input_mask = normalize(input_image, input_mask)\n",
        "\n",
        "  return input_image, input_mask\n",
        "\n",
        "TRAIN_LENGTH = info.splits['train'].num_examples\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
        "\n",
        "train_images = dataset['train'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_images = dataset['test'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "class Augment(tf.keras.layers.Layer):\n",
        "  def __init__(self, seed=(42,15,28)):\n",
        "    super().__init__()\n",
        "    # both use the same seed, so they'll make the same random changes.\n",
        "    self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed[0])\n",
        "    self.augment_labels = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed[0])\n",
        "\n",
        "    self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"vertical\", seed=seed[1])\n",
        "    self.augment_labels = tf.keras.layers.RandomFlip(mode=\"vertical\", seed=seed[1])\n",
        "\n",
        "    self.augment_inputs = tf.keras.layers.RandomRotation(factor=0.5, seed=seed[2])\n",
        "    self.augment_labels = tf.keras.layers.RandomRotation(factor=0.5, seed=seed[2])\n",
        "\n",
        "\n",
        "  def call(self, inputs, labels):\n",
        "    inputs = self.augment_inputs(inputs)\n",
        "    labels = self.augment_labels(labels)\n",
        "    return inputs, labels\n",
        "\n",
        "train_batches = (\n",
        "    train_images\n",
        "    .cache()\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .repeat()\n",
        "    .map(Augment())\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "test_batches = test_images.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "tpt1MdfNuiC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's take a look of the training input images and their corresponding masks."
      ],
      "metadata": {
        "id": "DnRQ-yY9bsK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display(display_list):\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    plt.title(title[i])\n",
        "    plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
        "    plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "for images, masks in train_batches.take(5):\n",
        "  sample_image, sample_mask = images[0], masks[0]\n",
        "  display([sample_image, sample_mask])"
      ],
      "metadata": {
        "id": "cyHplkTEHZif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now construct a U-Net for the image segmentation task.\n",
        "\n",
        "U-Net consists of an encoder (down-sampler) and a decoder (up-sampler). An encoder can be seen as a network mainly for feature extraction and a decoder can be seen as re-construct a image from the features. For the details of the U-Net, please see https://arxiv.org/abs/1505.04597\n",
        "\n",
        "Due to the limited training dataset, to ensure a better performance, we will do transfer learning. We will use a trained encoder provided by keras MobileNetV2 and only run training on an untrained decoder. For details of MobileNetV2, please see https://arxiv.org/abs/1801.04381\n",
        "\n",
        "* A question for you to consider: why for transfer learning we use an existing fixed trained encoder (the first half of the U-Net) and only train a decoder (the second half of the U-Net)? Why are we not doing the other way round (i.e. train an encoder and fix a decoder)?"
      ],
      "metadata": {
        "id": "i4rlgg98dCZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An untrainable encoder model using MobileNetV2."
      ],
      "metadata": {
        "id": "VVaczfuhpY7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n",
        "\n",
        "# Use the activations of these layers\n",
        "layer_names = [\n",
        "    'block_1_expand_relu',   # 64x64\n",
        "    'block_3_expand_relu',   # 32x32\n",
        "    'block_6_expand_relu',   # 16x16\n",
        "    'block_13_expand_relu',  # 8x8\n",
        "    'block_16_project',      # 4x4\n",
        "]\n",
        "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "# Create the feature extraction model\n",
        "down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
        "\n",
        "down_stack.trainable = False"
      ],
      "metadata": {
        "id": "pa6BFohKHeFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A trainable decoder model consisting 4 mini decoder block"
      ],
      "metadata": {
        "id": "a8QPszQmpjge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def DecoderMiniBlock(prev_layer_input, skip_layer_input, n_filters=32):\n",
        "    up = keras.layers.Conv2DTranspose(\n",
        "                 n_filters,\n",
        "                 (3,3),\n",
        "                 strides=(2,2),\n",
        "                 padding='same')(prev_layer_input)\n",
        "    merge = keras.layers.concatenate([up, skip_layer_input], axis=3)\n",
        "    conv = keras.layers.Conv2D(n_filters,\n",
        "                 3,\n",
        "                 activation='relu',\n",
        "                 padding='same',\n",
        "                 kernel_initializer='HeNormal')(merge)\n",
        "    conv = keras.layers.Conv2D(n_filters,\n",
        "                 3,\n",
        "                 activation='relu',\n",
        "                 padding='same',\n",
        "                 kernel_initializer='HeNormal')(conv)\n",
        "    return conv\n",
        "\n"
      ],
      "metadata": {
        "id": "zCoBIPvqpjDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's define our UNet and put the untrainable encoder and trainable decoder together."
      ],
      "metadata": {
        "id": "6sUJGdWQqMtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unet_model(output_channels:int):\n",
        "  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = down_stack(inputs)\n",
        "  x = skips[-1]\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for skip in skips:\n",
        "    x = DecoderMiniBlock(x,skip)\n",
        "\n",
        "  # This is the last layer of the model. Notice that kernel_size is 3 as our mask has 3 labels.\n",
        "  last = keras.layers.Conv2DTranspose(\n",
        "      filters=output_channels, kernel_size=3, strides=2,\n",
        "      padding='same')  #64x64 -> 128x128\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x,name='Oxford-IIIT_Pet_UNet')"
      ],
      "metadata": {
        "id": "4ldXwDgookHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_CLASSES = 3\n",
        "\n",
        "model = unet_model(output_channels=OUTPUT_CLASSES)\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "65FDD17OrljT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please print out the summary of the model. Could you notice the untrainable parameters of this UNet model? Where does these untrainable parameters come from? Do you expect this?"
      ],
      "metadata": {
        "id": "OYMLukPstVk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "dUiSmDpXtK7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To easily view the structure of this slightly complicated network, alternatively you can use *keras.utils.plot_model()*. Visually this may be easier to see for some people."
      ],
      "metadata": {
        "id": "8Qovq_C4toVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "2ZEMdzvHtjrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first examine the predictions of the UNet before training the model.\n",
        "\n",
        "The output of the UNet is in the size of 128 by 128 by 3. To display the mask, we apply *argmax* to the last channel so that the maximum pixel reading out of the three channels would return its channel index (either {0,1,2}).\n",
        "\n",
        "Without training the UNet, the returned predicted mask is random."
      ],
      "metadata": {
        "id": "L3zH_OKX1GJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(pred_mask,index):\n",
        "  pred_mask = tf.math.argmax(pred_mask, axis=-1)\n",
        "  pred_mask = pred_mask[..., tf.newaxis]\n",
        "  return pred_mask[index]\n",
        "\n",
        "def show_predictions(dataset=None, num=1,index = 0):\n",
        "  if dataset:\n",
        "    for image, mask in dataset.take(num):\n",
        "      pred_mask = model.predict(image)\n",
        "      display([image[index], mask[index], create_mask(pred_mask,index)])\n",
        "  else:\n",
        "    for images, masks in train_batches.take(1):\n",
        "      pred_mask = model.predict(images)\n",
        "      display([images[index], masks[index], create_mask(pred_mask,index)])\n",
        "\n",
        "show_predictions()"
      ],
      "metadata": {
        "id": "K-M9Qe_CuCsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HdHAU1Ib5ycz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "VAL_SUBSPLITS = 5\n",
        "VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n",
        "\n",
        "model_history = model.fit(train_batches, epochs=EPOCHS,\n",
        "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                          validation_steps=VALIDATION_STEPS,\n",
        "                          validation_data=test_batches,\n",
        "                          callbacks=[tensorboard_callback],)"
      ],
      "metadata": {
        "id": "cXDXKKfe0SQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please run the previous cell again to see how the predicted masks of training dataset using the trained model now look like.\n",
        "\n",
        "Please run the following cell to see how the predicted masks of the testing dataset using hte trained model now look like."
      ],
      "metadata": {
        "id": "dwAWbB3c6eaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_predictions(test_batches,1,5)"
      ],
      "metadata": {
        "id": "FcASXnuk6d8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 4: Train a neural network for image denoising\n"
      ],
      "metadata": {
        "id": "bq9WGBOq8TCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise we will train a self-supervised learning CNN called *noise2void*. This model trains only on 1 noisy image and outputs the clean version of the image. For details of noise2void, please see https://arxiv.org/abs/1811.10980\n",
        "\n",
        "We will first use StableDiffusion model from keras_cv to generate an image from the text of your choice. For details of StableDiffusion please see https://github.com/CompVis/stable-diffusion?tab=readme-ov-file#stable-diffusion-v1\n",
        "\n",
        "We will then add random noise to the image and use noise2void to remove the noise.\n",
        "\n",
        "You may wish to switch your runtime to GPU for this exercise."
      ],
      "metadata": {
        "id": "0fDbW7mm7Lwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the current noise2void is only supported for python 3.9, tensorflow 2.7 and 2.10, let's first install tensorflow v2.10. Note that you may need to restart your runtime to allow this installation."
      ],
      "metadata": {
        "id": "uuTGuU9-N52i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install python 3.9\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.9"
      ],
      "metadata": {
        "id": "X3Ci5u-HPJ0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.10"
      ],
      "metadata": {
        "id": "ycvXqHIbC0kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "StableDiffusion is available in *keras_cv*. Let's install *keras_cv*."
      ],
      "metadata": {
        "id": "ltY0SOuTPCLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow keras_cv --upgrade --quiet"
      ],
      "metadata": {
        "id": "_U0-E6lt8bcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import keras_cv\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "8ZUv6Sxr95SH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stablediffusion is an opensource text-to-image generation model. Please type in the text descibing the image you want to generate and run the cell. You may also adjust the size of your image by changing *img_width* and *img_height*. It may take a while to generate an image."
      ],
      "metadata": {
        "id": "Ic0ebUQk1Z9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras_cv.models.StableDiffusion(img_width=512, img_height=512)\n",
        "\n",
        "image = model.text_to_image(\"a high resolution image of an astronaut riding a horse\", batch_size=1)\n",
        "image = tf.cast(image, tf.float32) / 255\n",
        "plt.imshow(image[0])\n",
        "plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "-su2MWE098_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now add some noise to the image. You may adjust the noise level by adjusting the standard deviation of the noise generation setting."
      ],
      "metadata": {
        "id": "S7j3WnDE2XZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noisy_image = image+tf.random.normal(shape=image.shape,mean=0.0,stddev=0.1,)\n",
        "noisy_image = tf.clip_by_value(noisy_image, clip_value_min=0., clip_value_max=1.)\n",
        "plt.imshow(noisy_image[0])\n",
        "plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "dAU-W5H9CKcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now install the denoising model noise2void"
      ],
      "metadata": {
        "id": "mCEBSeCE2tXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install n2v\n"
      ],
      "metadata": {
        "id": "wyyOgCNS_fbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from n2v.models import N2VConfig, N2V\n",
        "import numpy as np\n",
        "from csbdeep.utils import plot_history\n",
        "from n2v.utils.n2v_utils import manipulate_val_data\n",
        "from n2v.internals.N2V_DataGenerator import N2V_DataGenerator\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "VtprXQnpHe6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to traing a model for denoising on a single noisy image, small patches need to be generated from the single image to form the training dataset. Here we selected a patch size of 64 by 64. Module *N2V_DataGenerator()* would help us to generate all the available patches."
      ],
      "metadata": {
        "id": "FI0FRRfp25_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = N2V_DataGenerator()\n",
        "noisy_image = np.expand_dims(np.asarray(noisy_image),axis=0)\n",
        "patch_shape=(64,64)\n",
        "patches = datagen.generate_patches_from_list(noisy_image, shape=patch_shape)"
      ],
      "metadata": {
        "id": "GnnWfDaXIaae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's split the training dataset from validation dataset. Ideally we have a larger portion for training and a smaller portion for testing."
      ],
      "metadata": {
        "id": "qFoJ8qzm3qNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = patches[:501]\n",
        "X_val = patches[501:]"
      ],
      "metadata": {
        "id": "U4t3oEcgKEQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is to show you an example of a training patch and a validation patch."
      ],
      "metadata": {
        "id": "gwBM-VE13-Lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14,7))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(X[0,...])\n",
        "plt.title('Training Patch');\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(X_val[0,...])\n",
        "plt.title('Validation Patch');"
      ],
      "metadata": {
        "id": "2dATlMZDKNwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the configuration we put in for this particular training."
      ],
      "metadata": {
        "id": "372TzikK4Fiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = N2VConfig(X, unet_kern_size=3,\n",
        "                   unet_n_first=64, unet_n_depth=3, train_steps_per_epoch=int(X.shape[0]/128), train_epochs=25, train_loss='mse',\n",
        "                   batch_norm=True, train_batch_size=128, n2v_perc_pix=0.198, n2v_patch_shape=(64, 64),\n",
        "                   n2v_manipulator='uniform_withCP', n2v_neighborhood_radius=5, single_net_per_channel=False)\n",
        "\n",
        "# Let's look at the parameters stored in the config-object.\n",
        "vars(config)\n",
        "\n",
        "model_name = 'n2v_2D'\n",
        "# the base directory in which our model will live\n",
        "basedir = 'models'\n",
        "# We are now creating our network model.\n",
        "model = N2V(config, model_name, basedir=basedir)"
      ],
      "metadata": {
        "id": "7RqXPuDAKSQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now train the model."
      ],
      "metadata": {
        "id": "b6J5rDwo4Tnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.train(X, X_val)"
      ],
      "metadata": {
        "id": "oDkLnpo8KdNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now use our trained N2V model to denoise our noisy image."
      ],
      "metadata": {
        "id": "XwsO7X954tMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(noisy_image[0,0], axes='YXC')\n",
        "plt.figure(figsize=(30,30))\n",
        "\n",
        "# We show the noisy input...\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(noisy_image[0,0] )\n",
        "plt.title('Input');\n",
        "\n",
        "# and the result.\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow( pred )\n",
        "plt.title('Prediction');"
      ],
      "metadata": {
        "id": "K0WcRlETK8ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 5: Train your neural network for flower classification"
      ],
      "metadata": {
        "id": "8shkZAv0NyVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, you will design and train a network for flower images classification. The dataset is downloaded from *tensorflow example_images/flower_photos* consists of more than 3,600 images and 5 classes of flowers.\n",
        "\n",
        "By now you should have taken your afternoon lecture and are aware that there are many choices on how you want to structure, regularise and train your neural network. You should also be aware of various ways to augment your training dataset.\n",
        "\n",
        "We will run a small competition within this class to see who will train a network provides a highest validation accuracy. Please include the metric *tf.keras.metrics.SparseCategoricalAccuracy()* in *model.compile()* to enter the competition as this metric (calculated from the validation dataset) will be used to evaluate your neural network performance for the competition. Whoever enters the competion has their permission to put their (preferred) name on the whiteboard. If you think your network performs better than anyone else's on the white board (including your own previous entry), please shout out and a demonstrator will come around to verify.\n",
        "\n",
        "The winner of this exercise will be awarded with a small gift."
      ],
      "metadata": {
        "id": "3BKtnRAo43sN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running this first cell will generate your basic training and validation dataset. To enter the competition, please do NOT modify this cell so that everyone starts with the same training/validation dataset split.\n",
        "\n",
        "You may augment your training dataset. Please do NOT modify your validation dataset."
      ],
      "metadata": {
        "id": "gSblotGJRdkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "import pathlib\n",
        "\n",
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "data_dir = tf.keras.utils.get_file('flower_photos.tar', origin=dataset_url, extract=True)\n",
        "data_dir = pathlib.Path(data_dir).with_suffix('')\n",
        "\n",
        "batch_size = 32\n",
        "img_height = 128\n",
        "img_width = 128\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "jXV1sRaiNx9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualising the dataset."
      ],
      "metadata": {
        "id": "ZDFtUdXITQXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(4):\n",
        "    ax = plt.subplot(2, 2, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "nItwVhaBRHmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All set! Now it is time to build and train your neural network for flower classification!"
      ],
      "metadata": {
        "id": "1WQMNatoUtZc"
      }
    }
  ]
}